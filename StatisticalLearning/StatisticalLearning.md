# 统计学习

### 梯度下降法

#### 介绍
$\quad$梯度下降法（gradient descent）或最速下降法（steepest descent）是求解无约束最优化问题的常见方法，是一种实现简单的迭代算法，每一步求解目标函数的梯度向量。当目标函数是凸函数时，梯度下降法的解是全局最优解，一般情况下，其解不保证是全局最优的。收敛速度也未必最快。    
#### 描述
$\quad$假设$f(x)$是$\mathbb{R}^{n}$上具有**一阶连续导数的函数**，求解的无约束优化问题是：$\mathop{min}\limits_{x\in\mathbb{R}^{n}}f(x),x^*$是目标函数的极小点。由于负梯度是使得函数下降最快的方向，以梯度方向更新$x$值。由于$f(x)$具有一阶连续偏导数，$k$次迭代值为$x^{(k)}$，则$f(x)$在$x^{(k)}$附近进行一阶泰勒展开$f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})$。$p_k$是搜索方向，$\lambda_k$是步长，由一维搜索确定。   
#### 算法实现
输入：目标函数$f(x)$，梯度函数$g(x)=\nabla f(x)$，计算精度$\varepsilon$；  
输出：$f(x)$的极小点$x^*$。  
(1)取初值 $x^{(0)}\in\mathbb{R}^{n}$，置 $k=0$  
(2)计算 $f(x^{(0)})$  
(3)计算梯度 $g_k =g(x^{(k)})$，当 $||g_k||<\varepsilon$ 时，停止迭代，令 $x^*=x^{(k)}$ ；否则，$p_k=-g(x^{(k)})$，求$\lambda_k$，使$f(x^{(k)}+\lambda_kp_k)=\mathop{min}\limits_{\lambda\geqslant 0}{f(x_{(k)}+\lambda p_k)}$  
(4)置$x^{(k+1)}=x^{(k)}+\lambda_kp_k$，计算$f(x^{(k+1)})$当$||f(x^{(k+1)}-f(x^{(k)}))||<\varepsilon$或$||x^{(k+1)}-x^{(k)}||<\varepsilon$时，停止迭代，$x^*=x^{(k+1)}$  
(5)否则 $k=k+1$，转到(3)。  
